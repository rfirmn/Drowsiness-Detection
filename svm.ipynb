{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from math import sqrt\n",
    "\n",
    "# ---------------------------\n",
    "# 1. KONFIGURASI ENV & MEDIAPIPE\n",
    "# ---------------------------\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Indeks Landmark MediaPipe\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "NOSE_TIP = 1\n",
    "CHIN = 152\n",
    "\n",
    "# Hyperparameters\n",
    "SEQUENCE_LENGTH = 30\n",
    "FPS = 30\n",
    "EAR_THRESHOLD = 0.22   \n",
    "MAR_THRESHOLD = 0.3    \n",
    "NODDING_THRESHOLD = 15 \n",
    "FACE_DETECTION_THRESHOLD = 0.05 \n",
    "\n",
    "# ---------------------------\n",
    "# 2. FUNGSI GEOMETRI & NUMERIK\n",
    "# ---------------------------\n",
    "def euclidean_distance(point1, point2):\n",
    "    return sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2 + (point1.z - point2.z)**2)\n",
    "\n",
    "def calculate_ear(landmarks, indices):\n",
    "    p = indices\n",
    "    v1 = euclidean_distance(landmarks[p[1]], landmarks[p[5]])\n",
    "    v2 = euclidean_distance(landmarks[p[2]], landmarks[p[4]])\n",
    "    h = euclidean_distance(landmarks[p[0]], landmarks[p[3]])\n",
    "    if h < 1e-6: return 0.0\n",
    "    return (v1 + v2) / (2.0 * h)\n",
    "\n",
    "def calculate_robust_mar(landmarks):\n",
    "    # Normalized MAR (Tahan Side View)\n",
    "    mouth_open = euclidean_distance(landmarks[13], landmarks[14])\n",
    "    face_vertical_ref = euclidean_distance(landmarks[NOSE_TIP], landmarks[CHIN])\n",
    "    if face_vertical_ref < 1e-6: return 0.0\n",
    "    return mouth_open / face_vertical_ref\n",
    "\n",
    "def estimate_pose_and_yaw(landmarks, w, h):\n",
    "    face_3d = np.array([\n",
    "        [0.0, 0.0, 0.0], [0.0, -150.0, -125.0], [0.0, 150.0, -125.0],\n",
    "        [-150.0, -125.0, 100.0], [150.0, -125.0, 100.0], [0.0, 0.0, 175.0]\n",
    "    ], dtype=np.float64)\n",
    "    face_2d = np.array([\n",
    "        [landmarks[1].x * w, landmarks[1].y * h],\n",
    "        [landmarks[61].x * w, landmarks[61].y * h],\n",
    "        [landmarks[291].x * w, landmarks[291].y * h],\n",
    "        [landmarks[33].x * w, landmarks[33].y * h],\n",
    "        [landmarks[263].x * w, landmarks[263].y * h],\n",
    "        [landmarks[10].x * w, landmarks[10].y * h]\n",
    "    ], dtype=np.float64)\n",
    "    cam_matrix = np.array([[w, 0, w/2], [0, w, h/2], [0, 0, 1]])\n",
    "    try:\n",
    "        success, rot, _ = cv2.solvePnP(face_3d, face_2d, cam_matrix, np.zeros((4,1)))\n",
    "        if success:\n",
    "            rmat, _ = cv2.Rodrigues(rot)\n",
    "            angles, _, _, _, _, _ = cv2.RQDecomp3x3(rmat)\n",
    "            return angles[0], -angles[1], angles[2]\n",
    "    except: pass\n",
    "    return 0.0, 0.0, 0.0\n",
    "\n",
    "# ---------------------------\n",
    "# 3. EKSTRAKSI FITUR (ADAPTIVE + FORWARD FILL)\n",
    "# ---------------------------\n",
    "def extract_features_adaptive(frame, prev_features=None):\n",
    "    # Resize untuk speed\n",
    "    target_w = 480\n",
    "    h_orig, w_orig = frame.shape[:2]\n",
    "    scale = target_w / w_orig if w_orig > target_w else 1.0\n",
    "    if scale < 1.0:\n",
    "        proc_frame = cv2.resize(frame, (0,0), fx=scale, fy=scale)\n",
    "    else:\n",
    "        proc_frame = frame\n",
    "        \n",
    "    rgb = cv2.cvtColor(proc_frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "    \n",
    "    # Forward Fill Logic\n",
    "    if not results.multi_face_landmarks:\n",
    "        if prev_features is not None:\n",
    "            safe = prev_features.copy()\n",
    "            safe[9] = 0.0 # Face flag 0 (Wajah hilang)\n",
    "            return safe\n",
    "        else:\n",
    "            d = np.zeros(12)\n",
    "            d[0], d[1] = 0.3, 0.3 \n",
    "            return d\n",
    "\n",
    "    lm = results.multi_face_landmarks[0].landmark\n",
    "    try:\n",
    "        pitch, yaw, roll = estimate_pose_and_yaw(lm, w_orig, h_orig)\n",
    "        ear_l = calculate_ear(lm, LEFT_EYE)\n",
    "        ear_r = calculate_ear(lm, RIGHT_EYE)\n",
    "        \n",
    "        # Side-Aware Logic\n",
    "        if yaw > 30: avg_ear = ear_r\n",
    "        elif yaw < -30: avg_ear = ear_l\n",
    "        else: avg_ear = (ear_l + ear_r) / 2.0\n",
    "        \n",
    "        mar = calculate_robust_mar(lm)\n",
    "        \n",
    "        perclos, blink_dur, yawn_dur = 0.0, 0.0, 0.0\n",
    "        if prev_features is not None:\n",
    "            if avg_ear < EAR_THRESHOLD:\n",
    "                perclos = min(prev_features[3] + 0.05, 1.0)\n",
    "                blink_dur = prev_features[4] + (1/FPS)\n",
    "            else:\n",
    "                perclos = max(prev_features[3] - 0.02, 0.0)\n",
    "                blink_dur = 0.0\n",
    "            \n",
    "            if mar > MAR_THRESHOLD:\n",
    "                yawn_dur = prev_features[5] + (1/FPS)\n",
    "            else:\n",
    "                yawn_dur = max(0, prev_features[5] - 0.1)\n",
    "\n",
    "        nodding = 1.0 if pitch > NODDING_THRESHOLD else 0.0\n",
    "        gaze = 1.0 - np.std([ear_l, ear_r])\n",
    "\n",
    "        feats = np.array([\n",
    "            ear_l, ear_r, mar, perclos, blink_dur, yawn_dur,\n",
    "            pitch, yaw, roll, 1.0, nodding, gaze\n",
    "        ])\n",
    "        return feats\n",
    "\n",
    "    except Exception:\n",
    "        return prev_features if prev_features is not None else np.zeros(12)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. SIGNAL SMOOTHING\n",
    "# ---------------------------\n",
    "def smooth_sequence_features(sequence_data, window_size=5):\n",
    "    \"\"\"\n",
    "    Smoothing sinyal EAR/MAR/Pose dalam satu sequence.\n",
    "    Penting untuk mengurangi jitter akibat kacamata.\n",
    "    \"\"\"\n",
    "    smoothed_data = sequence_data.copy()\n",
    "    # Indeks: EAR_L(0), EAR_R(1), MAR(2), Pitch(6), Yaw(7), Roll(8), Gaze(11)\n",
    "    target_indices = [0, 1, 2, 6, 7, 8, 11]\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    \n",
    "    for i in target_indices:\n",
    "        smoothed = np.convolve(sequence_data[:, i], kernel, mode='same')\n",
    "        smoothed_data[:, i] = smoothed\n",
    "    return smoothed_data\n",
    "\n",
    "# ---------------------------\n",
    "# 5. SEQUENCE GENERATOR (Group ID Included)\n",
    "# ---------------------------\n",
    "def create_sequences(video_path, label, subject_id):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    \n",
    "    if total_frames < SEQUENCE_LENGTH: return [], [], []\n",
    "    \n",
    "    start_indices = []\n",
    "    # Augmentasi Data (Overlap)\n",
    "    if label == 1: \n",
    "        stride = 5  # Drowsiness: Rapat\n",
    "        for i in range(0, total_frames - SEQUENCE_LENGTH, stride):\n",
    "            start_indices.append(i)\n",
    "    else: \n",
    "        stride = 15 # Normal: Sedang\n",
    "        for i in range(0, total_frames - SEQUENCE_LENGTH, stride):\n",
    "            start_indices.append(i)\n",
    "\n",
    "    # Limit max samples\n",
    "    if len(start_indices) > 50:\n",
    "        indices = np.linspace(0, len(start_indices)-1, 50, dtype=int)\n",
    "        start_indices = [start_indices[k] for k in indices]\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    groups = [] # Menyimpan ID Subjek\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    all_frames = []\n",
    "    while True:\n",
    "        ret, f = cap.read()\n",
    "        if not ret: break\n",
    "        all_frames.append(f)\n",
    "    cap.release()\n",
    "    \n",
    "    for start in start_indices:\n",
    "        chunk = all_frames[start : start+SEQUENCE_LENGTH]\n",
    "        if len(chunk) != SEQUENCE_LENGTH: continue\n",
    "        \n",
    "        seq_feats = []\n",
    "        prev = None\n",
    "        \n",
    "        for frame in chunk:\n",
    "            feats = extract_features_adaptive(frame, prev)\n",
    "            seq_feats.append(feats)\n",
    "            prev = feats\n",
    "        \n",
    "        # Apply Smoothing\n",
    "        seq_feats_np = np.array(seq_feats)\n",
    "        seq_feats_smoothed = smooth_sequence_features(seq_feats_np, window_size=5)\n",
    "        \n",
    "        sequences.append(seq_feats_smoothed)\n",
    "        labels.append(label)\n",
    "        groups.append(subject_id) # Simpan ID pemilik video ini\n",
    "        \n",
    "    return sequences, labels, groups\n",
    "\n",
    "# ---------------------------\n",
    "# 6. MAIN PROCESSOR (CV MODE)\n",
    "# ---------------------------\n",
    "def prepare_data_for_cv(metadata_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    \n",
    "    X_all = [] \n",
    "    y_all = []\n",
    "    groups_all = []\n",
    "    \n",
    "    print(\"Processing Numeric Dataset for Cross-Validation...\")\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if not os.path.exists(row['video_path']): continue\n",
    "        \n",
    "        # Proses video\n",
    "        s, l, g = create_sequences(row['video_path'], row['label'], row['subject_id'])\n",
    "        if s:\n",
    "            X_all.extend(s)\n",
    "            y_all.extend(l)\n",
    "            groups_all.extend(g)\n",
    "    \n",
    "    # Convert ke Numpy\n",
    "    X_all = np.array(X_all, dtype=np.float32) # (N, 30, 12)\n",
    "    y_all = np.array(y_all, dtype=np.int32)\n",
    "    groups_all = np.array(groups_all) # Array Subject ID\n",
    "    \n",
    "    print(f\"\\nCollected Sequences: {len(y_all)}\")\n",
    "    print(f\"Unique Subjects: {len(np.unique(groups_all))}\")\n",
    "    \n",
    "    # Filtering Data Rusak (Optional, Threshold rendah)\n",
    "    face_ratios = X_all[:, :, 9].mean(axis=1) # Index 9 is face_flag\n",
    "    mask = face_ratios >= FACE_DETECTION_THRESHOLD\n",
    "    \n",
    "    X_all = X_all[mask]\n",
    "    y_all = y_all[mask]\n",
    "    groups_all = groups_all[mask]\n",
    "    \n",
    "    print(f\"Post-Filtering Count: {len(y_all)}\")\n",
    "\n",
    "    # --- PENTING: JANGAN NORMALISASI DI SINI ---\n",
    "    # Untuk CV yang benar, normalisasi (hitung mean/std) harus dilakukan \n",
    "    # di dalam loop training per-fold agar tidak bocor.\n",
    "    # simpan data MENTAH (tapi sudah di-smooth).\n",
    "    \n",
    "    save_path = os.path.join(output_dir, \"numeric_data_cv.npz\")\n",
    "    np.savez_compressed(\n",
    "        save_path,\n",
    "        X=X_all,\n",
    "        y=y_all,\n",
    "        groups=groups_all\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ CV-Ready Dataset Saved to: {save_path}\")\n",
    "    print(\"Files contained: 'X', 'y', 'groups'\")\n",
    "    print(\"NOTE: Data belum dinormalisasi. Lakukan normalisasi di training loop.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_data_for_cv(\"metadata_process2.csv\", \"numeric_cv_data_non2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# ---------------------------\n",
    "# 1. KONFIGURASI\n",
    "# ---------------------------\n",
    "DATA_PATH = \"numeric_cv_data_non/numeric_data_cv.npz\" \n",
    "SUBMISSION_TEMPLATE = \"metadata_test.csv\" \n",
    "OUTPUT_CSV = \"submission_ml.csv\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2. FEATURE ENGINEERING (STATISTIKAL)\n",
    "# ---------------------------\n",
    "def extract_stats(seq_data):\n",
    "    \"\"\"\n",
    "    Mengubah sequence (30, 12) menjadi vector statistik (1, N_Features).\n",
    "    seq_data shape: (30, 12)\n",
    "    Fitur: Mean, Std, Max, Min, Skew, Kurtosis, Deltas\n",
    "    \"\"\"\n",
    "    # 12 Fitur Awal:\n",
    "    # 0:EAR_L, 1:EAR_R, 2:MAR, 3:Perclos, 4:Blink, 5:Yawn, \n",
    "    # 6-8:Pose, 9:Face, 10:Nod, 11:Gaze\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Ambil fitur kunci saja untuk statistik\n",
    "    # EAR Avg (0+1)/2, MAR (2), Pitch (6), Yaw (7), Roll (8)\n",
    "    ear_avg = (seq_data[:, 0] + seq_data[:, 1]) / 2.0\n",
    "    mar = seq_data[:, 2]\n",
    "    pitch = seq_data[:, 6]\n",
    "    yaw = seq_data[:, 7]\n",
    "    roll = seq_data[:, 8]\n",
    "    \n",
    "    # Kumpulan sinyal yang akan dianalisis\n",
    "    signals = [ear_avg, mar, pitch, yaw, roll]\n",
    "    names = ['ear', 'mar', 'pitch', 'yaw', 'roll']\n",
    "    \n",
    "    for sig in signals:\n",
    "        features.append(np.mean(sig))\n",
    "        features.append(np.std(sig))\n",
    "        features.append(np.max(sig))\n",
    "        features.append(np.min(sig))\n",
    "        features.append(np.max(sig) - np.min(sig)) # Range\n",
    "        # features.append(skew(sig)) # Opsional, kadang bikin noise\n",
    "    \n",
    "    # Fitur Khusus: Blink Count (Turun naik)\n",
    "    # Hitung berapa kali EAR turun dibawah 0.20\n",
    "    blink_count = np.sum(ear_avg < 0.20)\n",
    "    features.append(blink_count)\n",
    "    \n",
    "    # Fitur Khusus: Microsleep Duration\n",
    "    # Berapa frame berturut-turut EAR < 0.20\n",
    "    closed_frames = ear_avg < 0.20\n",
    "    max_closed = 0\n",
    "    current_closed = 0\n",
    "    for is_closed in closed_frames:\n",
    "        if is_closed:\n",
    "            current_closed += 1\n",
    "        else:\n",
    "            max_closed = max(max_closed, current_closed)\n",
    "            current_closed = 0\n",
    "    max_closed = max(max_closed, current_closed)\n",
    "    features.append(max_closed)\n",
    "    \n",
    "    # Fitur Khusus: Yawn Duration (MAR > 0.5)\n",
    "    yawn_frames = np.sum(mar > 0.5)\n",
    "    features.append(yawn_frames)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. PREPARE DATASET FOR ML\n",
    "# ---------------------------\n",
    "def prepare_ml_data(npz_path):\n",
    "    print(f\"Loading {npz_path}...\")\n",
    "    data = np.load(npz_path)\n",
    "    X_seq = data['X'] # (N, 30, 12)\n",
    "    y = data['y']\n",
    "    groups = data['groups']\n",
    "    \n",
    "    X_ml = []\n",
    "    print(\"Extracting Statistical Features...\")\n",
    "    for seq in tqdm(X_seq):\n",
    "        stats = extract_stats(seq)\n",
    "        X_ml.append(stats)\n",
    "        \n",
    "    return np.array(X_ml), y, groups\n",
    "\n",
    "# ---------------------------\n",
    "# 4. TRAINING (CROSS-VALIDATION)\n",
    "# ---------------------------\n",
    "def train_and_evaluate():\n",
    "    X, y, groups = prepare_ml_data(DATA_PATH)\n",
    "    print(f\"Feature Matrix Shape: {X.shape}\")\n",
    "    \n",
    "    # Scaler (Penting untuk SVM/KNN, kurang penting untuk XGB tapi bagus)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Simpan Scaler untuk inference nanti\n",
    "    joblib.dump(scaler, 'scaler_ml.pkl')\n",
    "    \n",
    "    # --- MODEL SELECTION ---\n",
    "    # Ganti model di sini untuk eksperimen\n",
    "    \n",
    "    # Opsi 1: XGBoost (Biasanya Paling Kuat)\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Training using: {model.__class__.__name__}\")\n",
    "    \n",
    "    # GroupKFold untuk validasi yang jujur\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    scores = []\n",
    "    \n",
    "    # Kita latih 5 model dan simpan semuanya untuk Voting\n",
    "    models_list = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_scaled, y, groups)):\n",
    "        X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "        X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, preds, average='macro')\n",
    "        scores.append(f1)\n",
    "        \n",
    "        print(f\"Fold {fold+1} F1: {f1:.4f}\")\n",
    "        print(classification_report(y_val, preds, target_names=['Non', 'Drowsy']))\n",
    "        \n",
    "        # Simpan model per fold\n",
    "        models_list.append(model)\n",
    "        joblib.dump(model, f'model_ml_fold_{fold+1}.pkl')\n",
    "        \n",
    "    print(f\"\\nüèÜ Average CV Score: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "    return models_list, scaler\n",
    "\n",
    "# ---------------------------\n",
    "# 5. INFERENCE ON TEST DATA\n",
    "# ---------------------------\n",
    "def run_test_inference(models_list, scaler):\n",
    "    # Kita butuh memproses data test dengan cara yang SAMA (Statistical)\n",
    "    \n",
    "    TEST_NPZ = \"test_data_numeric_output/test_numeric_sliding.npz\"\n",
    "    if not os.path.exists(TEST_NPZ):\n",
    "        print(\"‚ùå Please run preprocess_test_numeric.py first!\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nLoading Test Data...\")\n",
    "    data = np.load(TEST_NPZ)\n",
    "    X_test_seq = data['X_test'] # (N, 30, 12)\n",
    "    video_ids = data['ids']\n",
    "    \n",
    "    # Feature Engineering\n",
    "    X_test_ml = []\n",
    "    for seq in X_test_seq:\n",
    "        X_test_ml.append(extract_stats(seq))\n",
    "    X_test_ml = np.array(X_test_ml)\n",
    "    \n",
    "    # Scaling\n",
    "    X_test_scaled = scaler.transform(X_test_ml)\n",
    "    \n",
    "    # Ensemble Prediction (Voting)\n",
    "    final_probs = np.zeros((len(X_test_scaled), 2))\n",
    "    \n",
    "    for model in models_list:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X_test_scaled)\n",
    "        else:\n",
    "            # Fallback untuk model tanpa proba (jarang)\n",
    "            p = model.predict(X_test_scaled)\n",
    "            probs = np.zeros((len(p), 2))\n",
    "            for i, val in enumerate(p): probs[i, val] = 1.0\n",
    "            \n",
    "        final_probs += probs\n",
    "        \n",
    "    final_preds = np.argmax(final_probs, axis=1)\n",
    "    \n",
    "    # Save CSV\n",
    "    df = pd.DataFrame({'video_id': video_ids, 'label': final_preds})\n",
    "    df['video_id'] = df['video_id'].astype(str).str.replace('.mp4', '', regex=False)\n",
    "    df = df.sort_values('video_id')\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Submission Saved: {OUTPUT_CSV}\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Train\n",
    "    trained_models, fitted_scaler = train_and_evaluate()\n",
    "    \n",
    "    # 2. Predict\n",
    "    run_test_inference(trained_models, fitted_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2cf9e2",
   "metadata": {},
   "source": [
    "Loading numeric_cv_data_non/numeric_data_cv.npz...\n",
    "Extracting Statistical Features...\n",
    "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4022/4022 [00:00<00:00, 11466.16it/s]\n",
    "Feature Matrix Shape: (4022, 28)\n",
    "Training using: XGBClassifier\n",
    "Fold 1 F1: 0.7929\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         Non       0.87      0.86      0.87       549\n",
    "      Drowsy       0.71      0.73      0.72       252\n",
    "\n",
    "    accuracy                           0.82       801\n",
    "   macro avg       0.79      0.79      0.79       801\n",
    "weighted avg       0.82      0.82      0.82       801\n",
    "\n",
    "Fold 2 F1: 0.8730\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         Non       0.93      0.91      0.92       570\n",
    "      Drowsy       0.81      0.84      0.83       253\n",
    "\n",
    "    accuracy                           0.89       823\n",
    "   macro avg       0.87      0.88      0.87       823\n",
    "weighted avg       0.89      0.89      0.89       823\n",
    "\n",
    "Fold 3 F1: 0.7050\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         Non       0.78      0.87      0.82       532\n",
    "      Drowsy       0.68      0.52      0.59       280\n",
    "\n",
    "    accuracy                           0.75       812\n",
    "   macro avg       0.73      0.70      0.71       812\n",
    "weighted avg       0.74      0.75      0.74       812\n",
    "\n",
    "Fold 4 F1: 0.7555\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         Non       0.76      0.87      0.81       469\n",
    "      Drowsy       0.78      0.63      0.70       344\n",
    "\n",
    "    accuracy                           0.77       813\n",
    "   macro avg       0.77      0.75      0.76       813\n",
    "weighted avg       0.77      0.77      0.76       813\n",
    "\n",
    "Fold 5 F1: 0.8896\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         Non       0.90      0.95      0.92       487\n",
    "      Drowsy       0.90      0.82      0.86       286\n",
    "\n",
    "    accuracy                           0.90       773\n",
    "   macro avg       0.90      0.88      0.89       773\n",
    "weighted avg       0.90      0.90      0.90       773\n",
    "\n",
    "\n",
    "üèÜ Average CV Score: 0.8032 (+/- 0.0698)\n",
    "\n",
    "Loading Test Data...\n",
    "‚úÖ Submission Saved: submission_ml.csv\n",
    "label\n",
    "0    367\n",
    "1    235\n",
    "Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading numeric_cv_data_non2/numeric_data_cv.npz...\n",
      "Extracting Statistical Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3189/3189 [00:00<00:00, 9222.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (3189, 34)\n",
      "Training using: SVC\n",
      "\n",
      "Starting Cross-Validation...\n",
      "Fold 1 F1: 0.7559\n",
      "Fold 2 F1: 0.7712\n",
      "Fold 3 F1: 0.8149\n",
      "Fold 4 F1: 0.7506\n",
      "Fold 5 F1: 0.8263\n",
      "\n",
      "üèÜ Average CV Score: 0.7838 (+/- 0.0310)\n",
      "\n",
      "Loading Test Data from test_data_numeric_output/test_numeric_sliding.npz...\n",
      "Extracting Test Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 602/602 [00:00<00:00, 8192.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with Ensemble of CV Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Submission Saved: submission_svm3.csv\n",
      "\n",
      "Class Distribution:\n",
      "label\n",
      "0    395\n",
      "1    207\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "# ---------------------------\n",
    "# 1. KONFIGURASI\n",
    "# ---------------------------\n",
    "DATA_PATH = \"numeric_cv_data_non2/numeric_data_cv.npz\" \n",
    "TEST_NPZ_PATH = \"test_data_numeric_output/test_numeric_sliding.npz\" \n",
    "OUTPUT_CSV = \"submission_svm3.csv\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2. FEATURE ENGINEERING (STATISTIKAL)\n",
    "# ---------------------------\n",
    "def extract_stats(seq_data):\n",
    "    \"\"\"\n",
    "    Mengubah sequence (30, 12) menjadi vector statistik (1, N_Features).\n",
    "    seq_data shape: (30, 12)\n",
    "    Fitur: Mean, Std, Max, Min, Range, Blink Count, Microsleep Duration\n",
    "    \"\"\"\n",
    "    # 12 Fitur Awal:\n",
    "    # 0:EAR_L, 1:EAR_R, 2:MAR, 3:Perclos, 4:Blink, 5:Yawn, \n",
    "    # 6-8:Pose, 9:Face, 10:Nod, 11:Gaze\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Ambil fitur kunci saja untuk statistik\n",
    "    ear_avg = (seq_data[:, 0] + seq_data[:, 1]) / 2.0\n",
    "    mar = seq_data[:, 2]\n",
    "    pitch = seq_data[:, 6]\n",
    "    yaw = seq_data[:, 7]\n",
    "    roll = seq_data[:, 8]\n",
    "    gaze = seq_data[:, 11]\n",
    "    \n",
    "    # Kumpulan sinyal yang akan dianalisis\n",
    "    signals = [ear_avg, mar, pitch, yaw, roll, gaze]\n",
    "    \n",
    "    for sig in signals:\n",
    "        features.append(np.mean(sig))\n",
    "        features.append(np.std(sig))\n",
    "        features.append(np.max(sig))\n",
    "        features.append(np.min(sig))\n",
    "        features.append(np.max(sig) - np.min(sig)) # Range\n",
    "    \n",
    "    # --- FITUR KHUSUS (DOMAIN KNOWLEDGE) ---\n",
    "    \n",
    "    # 1. Blink Count (Turun naik) - Hitung berapa kali EAR turun dibawah 0.20\n",
    "    # Menggunakan diff untuk menghitung transisi open->close\n",
    "    binary_ear = (ear_avg < 0.20).astype(int)\n",
    "    transitions = np.diff(binary_ear)\n",
    "    blink_count = np.sum(transitions == 1) # Hitung transisi masuk ke fase tertutup\n",
    "    features.append(blink_count)\n",
    "    \n",
    "    # 2. Microsleep Duration (Max consecutive frames mata tertutup)\n",
    "    closed_frames = ear_avg < 0.20\n",
    "    max_closed = 0\n",
    "    current_closed = 0\n",
    "    for is_closed in closed_frames:\n",
    "        if is_closed:\n",
    "            current_closed += 1\n",
    "        else:\n",
    "            max_closed = max(max_closed, current_closed)\n",
    "            current_closed = 0\n",
    "    max_closed = max(max_closed, current_closed)\n",
    "    features.append(max_closed)\n",
    "    \n",
    "    # 3. Yawn Duration (MAR > 0.5)\n",
    "    yawn_frames = np.sum(mar > 0.5)\n",
    "    features.append(yawn_frames)\n",
    "    \n",
    "    # 4. Head Nodding (Pitch > Threshold secara konsisten)\n",
    "    nodding_frames = np.sum(pitch > 15)\n",
    "    features.append(nodding_frames)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. PREPARE DATASET FOR ML\n",
    "# ---------------------------\n",
    "def prepare_ml_data(npz_path):\n",
    "    print(f\"Loading {npz_path}...\")\n",
    "    data = np.load(npz_path)\n",
    "    X_seq = data['X'] # (N, 30, 12)\n",
    "    y = data['y']\n",
    "    groups = data['groups']\n",
    "    \n",
    "    X_ml = []\n",
    "    print(\"Extracting Statistical Features...\")\n",
    "    for seq in tqdm(X_seq):\n",
    "        stats = extract_stats(seq)\n",
    "        X_ml.append(stats)\n",
    "        \n",
    "    return np.array(X_ml), y, groups\n",
    "\n",
    "# ---------------------------\n",
    "# 4. TRAINING (CROSS-VALIDATION)\n",
    "# ---------------------------\n",
    "def train_and_evaluate():\n",
    "    X, y, groups = prepare_ml_data(DATA_PATH)\n",
    "    print(f\"Feature Matrix Shape: {X.shape}\")\n",
    "    \n",
    "    # Menggunakan RobustScaler karena fitur statistik sering punya outlier\n",
    "    scaler = RobustScaler() \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Simpan Scaler\n",
    "    joblib.dump(scaler, 'scaler_ml.pkl')\n",
    "    \n",
    "    # --- PILIHAN MODEL ---\n",
    "    \n",
    "    # 1. SVM (Support Vector Machine) - SANGAT DIREKOMENDASIKAN\n",
    "    # class_weight='balanced' penting untuk dataset kecil/imbalance\n",
    "    svm_model = SVC(\n",
    "        kernel='rbf', \n",
    "        C=10,             # Regularization parameter\n",
    "        gamma='scale',    # Kernel coefficient\n",
    "        probability=True, # Agar bisa predict_proba untuk ensemble\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 2. Random Forest - Sebagai pembanding/Ensemble\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # --- MODEL UTAMA YANG AKAN DILATIH ---\n",
    "    model = svm_model\n",
    "    \n",
    "    print(f\"Training using: {model.__class__.__name__}\")\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    scores = []\n",
    "    models_list = []\n",
    "    \n",
    "    print(\"\\nStarting Cross-Validation...\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_scaled, y, groups)):\n",
    "        X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "        X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, preds, average='macro')\n",
    "        scores.append(f1)\n",
    "        \n",
    "        print(f\"Fold {fold+1} F1: {f1:.4f}\")\n",
    "        # print(classification_report(y_val, preds, target_names=['Non', 'Drowsy']))\n",
    "        \n",
    "        # Simpan model (Kita butuh clone fresh model atau fit ulang, \n",
    "        # tapi untuk sklearn simple fit sudah overwrite)\n",
    "        # Kita simpan ke disk agar aman\n",
    "        joblib.dump(model, f'model_ml_fold_{fold+1}.pkl')\n",
    "        models_list.append(f'model_ml_fold_{fold+1}.pkl') # Simpan path/nama\n",
    "        \n",
    "    print(f\"\\nüèÜ Average CV Score: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "    \n",
    "    return models_list, scaler\n",
    "\n",
    "# ---------------------------\n",
    "# 5. INFERENCE ON TEST DATA\n",
    "# ---------------------------\n",
    "def run_test_inference(model_paths, scaler):\n",
    "    if not os.path.exists(TEST_NPZ_PATH):\n",
    "        print(f\"‚ùå Test data not found at: {TEST_NPZ_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nLoading Test Data from {TEST_NPZ_PATH}...\")\n",
    "    data = np.load(TEST_NPZ_PATH)\n",
    "    \n",
    "    # Handle variasi nama key pada file npz (kadang X_test, kadang X_test_num)\n",
    "    if 'X_test' in data:\n",
    "        X_test_seq = data['X_test']\n",
    "    elif 'X_test_num' in data:\n",
    "        X_test_seq = data['X_test_num']\n",
    "    else:\n",
    "        # Fallback coba ambil array pertama\n",
    "        X_test_seq = data[data.files[0]]\n",
    "        \n",
    "    video_ids = data['ids'] if 'ids' in data else data['video_ids']\n",
    "    \n",
    "    print(\"Extracting Test Features...\")\n",
    "    X_test_ml = []\n",
    "    for seq in tqdm(X_test_seq):\n",
    "        stats = extract_stats(seq)\n",
    "        X_test_ml.append(stats)\n",
    "    X_test_ml = np.array(X_test_ml)\n",
    "    \n",
    "    # Scaling\n",
    "    X_test_scaled = scaler.transform(X_test_ml)\n",
    "    \n",
    "    # Ensemble Prediction (Average Probability dari semua Fold)\n",
    "    print(\"Predicting with Ensemble of CV Models...\")\n",
    "    final_probs = np.zeros((len(X_test_scaled), 2))\n",
    "    \n",
    "    for m_path in model_paths:\n",
    "        model = joblib.load(m_path)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X_test_scaled)\n",
    "        else:\n",
    "            p = model.predict(X_test_scaled)\n",
    "            probs = np.zeros((len(p), 2))\n",
    "            for i, val in enumerate(p): probs[i, val] = 1.0\n",
    "        final_probs += probs\n",
    "        \n",
    "    final_preds = np.argmax(final_probs, axis=1)\n",
    "    \n",
    "    # Save CSV\n",
    "    df = pd.DataFrame({'video_id': video_ids, 'label': final_preds})\n",
    "    # Cleaning ID\n",
    "    df['video_id'] = df['video_id'].astype(str).str.replace('.mp4', '', regex=False)\n",
    "    df = df.sort_values('video_id')\n",
    "    \n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Submission Saved: {OUTPUT_CSV}\")\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Train SVM/RF\n",
    "    trained_model_paths, fitted_scaler = train_and_evaluate()\n",
    "    \n",
    "    # 2. Predict Test Data\n",
    "    run_test_inference(trained_model_paths, fitted_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading numeric_cv_data_non2/numeric_data_cv.npz...\n",
      "Extracting Statistical Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3189/3189 [00:00<00:00, 10650.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (3189, 28)\n",
      "Training using: XGBClassifier\n",
      "Fold 1 F1: 0.7265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Non       0.71      0.80      0.75       332\n",
      "      Drowsy       0.75      0.65      0.70       307\n",
      "\n",
      "    accuracy                           0.73       639\n",
      "   macro avg       0.73      0.73      0.73       639\n",
      "weighted avg       0.73      0.73      0.73       639\n",
      "\n",
      "Fold 2 F1: 0.8339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Non       0.84      0.85      0.85       331\n",
      "      Drowsy       0.83      0.82      0.82       287\n",
      "\n",
      "    accuracy                           0.83       618\n",
      "   macro avg       0.83      0.83      0.83       618\n",
      "weighted avg       0.83      0.83      0.83       618\n",
      "\n",
      "Fold 3 F1: 0.8618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Non       0.90      0.88      0.89       385\n",
      "      Drowsy       0.82      0.85      0.84       264\n",
      "\n",
      "    accuracy                           0.87       649\n",
      "   macro avg       0.86      0.86      0.86       649\n",
      "weighted avg       0.87      0.87      0.87       649\n",
      "\n",
      "Fold 4 F1: 0.7869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Non       0.77      0.95      0.85       374\n",
      "      Drowsy       0.89      0.61      0.72       272\n",
      "\n",
      "    accuracy                           0.80       646\n",
      "   macro avg       0.83      0.78      0.79       646\n",
      "weighted avg       0.82      0.80      0.80       646\n",
      "\n",
      "Fold 5 F1: 0.8376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Non       0.87      0.85      0.86       371\n",
      "      Drowsy       0.80      0.82      0.81       266\n",
      "\n",
      "    accuracy                           0.84       637\n",
      "   macro avg       0.84      0.84      0.84       637\n",
      "weighted avg       0.84      0.84      0.84       637\n",
      "\n",
      "\n",
      "üèÜ Average CV Score: 0.8094 (+/- 0.0480)\n",
      "\n",
      "Loading Test Data...\n",
      "‚úÖ Submission Saved: submission_ml2.csv\n",
      "label\n",
      "0    368\n",
      "1    234\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# ---------------------------\n",
    "# 1. KONFIGURASI\n",
    "# ---------------------------\n",
    "DATA_PATH = \"numeric_cv_data_non2/numeric_data_cv.npz\" \n",
    "SUBMISSION_TEMPLATE = \"metadata_test.csv\"\n",
    "OUTPUT_CSV = \"submission_ml2.csv\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2. FEATURE ENGINEERING (STATISTIKAL)\n",
    "# ---------------------------\n",
    "def extract_stats(seq_data):\n",
    "    \"\"\"\n",
    "    Mengubah sequence (30, 12) menjadi vector statistik (1, N_Features).\n",
    "    seq_data shape: (30, 12)\n",
    "    Fitur: Mean, Std, Max, Min, Skew, Kurtosis, Deltas\n",
    "    \"\"\"\n",
    "    # 12 Fitur Awal:\n",
    "    # 0:EAR_L, 1:EAR_R, 2:MAR, 3:Perclos, 4:Blink, 5:Yawn, \n",
    "    # 6-8:Pose, 9:Face, 10:Nod, 11:Gaze\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Ambil fitur kunci saja untuk statistik\n",
    "    # EAR Avg (0+1)/2, MAR (2), Pitch (6), Yaw (7), Roll (8)\n",
    "    ear_avg = (seq_data[:, 0] + seq_data[:, 1]) / 2.0\n",
    "    mar = seq_data[:, 2]\n",
    "    pitch = seq_data[:, 6]\n",
    "    yaw = seq_data[:, 7]\n",
    "    roll = seq_data[:, 8]\n",
    "    \n",
    "    # Kumpulan sinyal yang akan dianalisis\n",
    "    signals = [ear_avg, mar, pitch, yaw, roll]\n",
    "    names = ['ear', 'mar', 'pitch', 'yaw', 'roll']\n",
    "    \n",
    "    for sig in signals:\n",
    "        features.append(np.mean(sig))\n",
    "        features.append(np.std(sig))\n",
    "        features.append(np.max(sig))\n",
    "        features.append(np.min(sig))\n",
    "        features.append(np.max(sig) - np.min(sig)) # Range\n",
    "        # features.append(skew(sig)) # Opsional, kadang bikin noise\n",
    "    \n",
    "    # Fitur Khusus: Blink Count (Turun naik)\n",
    "    # Hitung berapa kali EAR turun dibawah 0.20\n",
    "    blink_count = np.sum(ear_avg < 0.20)\n",
    "    features.append(blink_count)\n",
    "    \n",
    "    # Fitur Khusus: Microsleep Duration\n",
    "    # Berapa frame berturut-turut EAR < 0.20\n",
    "    closed_frames = ear_avg < 0.20\n",
    "    max_closed = 0\n",
    "    current_closed = 0\n",
    "    for is_closed in closed_frames:\n",
    "        if is_closed:\n",
    "            current_closed += 1\n",
    "        else:\n",
    "            max_closed = max(max_closed, current_closed)\n",
    "            current_closed = 0\n",
    "    max_closed = max(max_closed, current_closed)\n",
    "    features.append(max_closed)\n",
    "    \n",
    "    # Fitur Khusus: Yawn Duration (MAR > 0.5)\n",
    "    yawn_frames = np.sum(mar > 0.5)\n",
    "    features.append(yawn_frames)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. PREPARE DATASET FOR ML\n",
    "# ---------------------------\n",
    "def prepare_ml_data(npz_path):\n",
    "    print(f\"Loading {npz_path}...\")\n",
    "    data = np.load(npz_path)\n",
    "    X_seq = data['X'] # (N, 30, 12)\n",
    "    y = data['y']\n",
    "    groups = data['groups']\n",
    "    \n",
    "    X_ml = []\n",
    "    print(\"Extracting Statistical Features...\")\n",
    "    for seq in tqdm(X_seq):\n",
    "        stats = extract_stats(seq)\n",
    "        X_ml.append(stats)\n",
    "        \n",
    "    return np.array(X_ml), y, groups\n",
    "\n",
    "# ---------------------------\n",
    "# 4. TRAINING (CROSS-VALIDATION)\n",
    "# ---------------------------\n",
    "def train_and_evaluate():\n",
    "    X, y, groups = prepare_ml_data(DATA_PATH)\n",
    "    print(f\"Feature Matrix Shape: {X.shape}\")\n",
    "    \n",
    "    # Scaler (Penting untuk SVM/KNN, kurang penting untuk XGB tapi bagus)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Simpan Scaler untuk inference nanti\n",
    "    joblib.dump(scaler, 'scaler_ml.pkl')\n",
    "    \n",
    "    # --- MODEL SELECTION ---\n",
    "    # Ganti model di sini untuk eksperimen\n",
    "    \n",
    "    # Opsi 1: XGBoost (Biasanya Paling Kuat)\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Training using: {model.__class__.__name__}\")\n",
    "    \n",
    "    # GroupKFold untuk validasi yang jujur\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    scores = []\n",
    "    \n",
    "    # Kita latih 5 model dan simpan semuanya untuk Voting\n",
    "    models_list = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_scaled, y, groups)):\n",
    "        X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "        X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, preds, average='macro')\n",
    "        scores.append(f1)\n",
    "        \n",
    "        print(f\"Fold {fold+1} F1: {f1:.4f}\")\n",
    "        print(classification_report(y_val, preds, target_names=['Non', 'Drowsy']))\n",
    "        \n",
    "        # Simpan model per fold\n",
    "        models_list.append(model)\n",
    "        joblib.dump(model, f'model_ml_fold_{fold+1}.pkl')\n",
    "        \n",
    "    print(f\"\\nüèÜ Average CV Score: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "    return models_list, scaler\n",
    "\n",
    "# ---------------------------\n",
    "# 5. INFERENCE ON TEST DATA\n",
    "# ---------------------------\n",
    "def run_test_inference(models_list, scaler):\n",
    "    # Kita butuh memproses data test dengan cara yang SAMA (Statistical)\n",
    "    \n",
    "    TEST_NPZ = \"test_data_numeric_output/test_numeric_sliding.npz\"\n",
    "    if not os.path.exists(TEST_NPZ):\n",
    "        print(\"‚ùå Please run preprocess_test_numeric.py first!\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nLoading Test Data...\")\n",
    "    data = np.load(TEST_NPZ)\n",
    "    X_test_seq = data['X_test'] # (N, 30, 12)\n",
    "    video_ids = data['ids']\n",
    "    \n",
    "    # Feature Engineering\n",
    "    X_test_ml = []\n",
    "    for seq in X_test_seq:\n",
    "        X_test_ml.append(extract_stats(seq))\n",
    "    X_test_ml = np.array(X_test_ml)\n",
    "    \n",
    "    # Scaling\n",
    "    X_test_scaled = scaler.transform(X_test_ml)\n",
    "    \n",
    "    # Ensemble Prediction (Voting)\n",
    "    final_probs = np.zeros((len(X_test_scaled), 2))\n",
    "    \n",
    "    for model in models_list:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X_test_scaled)\n",
    "        else:\n",
    "            # Fallback untuk model tanpa proba (jarang)\n",
    "            p = model.predict(X_test_scaled)\n",
    "            probs = np.zeros((len(p), 2))\n",
    "            for i, val in enumerate(p): probs[i, val] = 1.0\n",
    "            \n",
    "        final_probs += probs\n",
    "        \n",
    "    final_preds = np.argmax(final_probs, axis=1)\n",
    "    \n",
    "    # Save CSV\n",
    "    df = pd.DataFrame({'video_id': video_ids, 'label': final_preds})\n",
    "    df['video_id'] = df['video_id'].astype(str).str.replace('.mp4', '', regex=False)\n",
    "    df = df.sort_values('video_id')\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Submission Saved: {OUTPUT_CSV}\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Train\n",
    "    trained_models, fitted_scaler = train_and_evaluate()\n",
    "    \n",
    "    # 2. Predict\n",
    "    run_test_inference(trained_models, fitted_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading numeric_cv_data_non2/numeric_data_cv.npz...\n",
      "Extracting Statistical Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3189/3189 [00:00<00:00, 8891.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (3189, 34)\n",
      "Training using: VotingClassifier (SVM + RF + KNN)\n",
      "\n",
      "Starting Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1: 0.7610\n",
      "Fold 2 F1: 0.7769\n",
      "Fold 3 F1: 0.8222\n",
      "Fold 4 F1: 0.7553\n",
      "Fold 5 F1: 0.8476\n",
      "\n",
      "üèÜ Average CV Score: 0.7926 (+/- 0.0362)\n",
      "\n",
      "Loading Test Data from test_data_numeric_output/test_numeric_sliding.npz...\n",
      "Extracting Test Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 602/602 [00:00<00:00, 7524.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with Ensemble of CV Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Submission Saved: submission_ensemble_voting.csv\n",
      "\n",
      "Class Distribution:\n",
      "label\n",
      "0    411\n",
      "1    191\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "# ---------------------------\n",
    "# 1. KONFIGURASI\n",
    "# ---------------------------\n",
    "DATA_PATH = \"numeric_cv_data_non2/numeric_data_cv.npz\" \n",
    "TEST_NPZ_PATH = \"test_data_numeric_output/test_numeric_sliding.npz\" \n",
    "OUTPUT_CSV = \"submission_ensemble_voting.csv\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2. FEATURE ENGINEERING (STATISTIKAL)\n",
    "# ---------------------------\n",
    "def extract_stats(seq_data):\n",
    "    \"\"\"\n",
    "    Mengubah sequence (30, 12) menjadi vector statistik (1, N_Features).\n",
    "    seq_data shape: (30, 12)\n",
    "    Fitur: Mean, Std, Max, Min, Range, Blink Count, Microsleep Duration\n",
    "    \"\"\"\n",
    "    # 12 Fitur Awal:\n",
    "    # 0:EAR_L, 1:EAR_R, 2:MAR, 3:Perclos, 4:Blink, 5:Yawn, \n",
    "    # 6-8:Pose, 9:Face, 10:Nod, 11:Gaze\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Ambil fitur kunci saja untuk statistik\n",
    "    ear_avg = (seq_data[:, 0] + seq_data[:, 1]) / 2.0\n",
    "    mar = seq_data[:, 2]\n",
    "    pitch = seq_data[:, 6]\n",
    "    yaw = seq_data[:, 7]\n",
    "    roll = seq_data[:, 8]\n",
    "    gaze = seq_data[:, 11]\n",
    "    \n",
    "    # Kumpulan sinyal yang akan dianalisis\n",
    "    signals = [ear_avg, mar, pitch, yaw, roll, gaze]\n",
    "    \n",
    "    for sig in signals:\n",
    "        features.append(np.mean(sig))\n",
    "        features.append(np.std(sig))\n",
    "        features.append(np.max(sig))\n",
    "        features.append(np.min(sig))\n",
    "        features.append(np.max(sig) - np.min(sig)) # Range\n",
    "    \n",
    "    # --- FITUR KHUSUS (DOMAIN KNOWLEDGE) ---\n",
    "    \n",
    "    # 1. Blink Count (Turun naik) - Hitung berapa kali EAR turun dibawah 0.20\n",
    "    # Menggunakan diff untuk menghitung transisi open->close\n",
    "    binary_ear = (ear_avg < 0.20).astype(int)\n",
    "    transitions = np.diff(binary_ear)\n",
    "    blink_count = np.sum(transitions == 1) # Hitung transisi masuk ke fase tertutup\n",
    "    features.append(blink_count)\n",
    "    \n",
    "    # 2. Microsleep Duration (Max consecutive frames mata tertutup)\n",
    "    closed_frames = ear_avg < 0.20\n",
    "    max_closed = 0\n",
    "    current_closed = 0\n",
    "    for is_closed in closed_frames:\n",
    "        if is_closed:\n",
    "            current_closed += 1\n",
    "        else:\n",
    "            max_closed = max(max_closed, current_closed)\n",
    "            current_closed = 0\n",
    "    max_closed = max(max_closed, current_closed)\n",
    "    features.append(max_closed)\n",
    "    \n",
    "    # 3. Yawn Duration (MAR > 0.5)\n",
    "    yawn_frames = np.sum(mar > 0.5)\n",
    "    features.append(yawn_frames)\n",
    "    \n",
    "    # 4. Head Nodding (Pitch > Threshold secara konsisten)\n",
    "    nodding_frames = np.sum(pitch > 15)\n",
    "    features.append(nodding_frames)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. PREPARE DATASET FOR ML\n",
    "# ---------------------------\n",
    "def prepare_ml_data(npz_path):\n",
    "    print(f\"Loading {npz_path}...\")\n",
    "    data = np.load(npz_path)\n",
    "    X_seq = data['X'] # (N, 30, 12)\n",
    "    y = data['y']\n",
    "    groups = data['groups']\n",
    "    \n",
    "    X_ml = []\n",
    "    print(\"Extracting Statistical Features...\")\n",
    "    for seq in tqdm(X_seq):\n",
    "        stats = extract_stats(seq)\n",
    "        X_ml.append(stats)\n",
    "        \n",
    "    return np.array(X_ml), y, groups\n",
    "\n",
    "# ---------------------------\n",
    "# 4. TRAINING (CROSS-VALIDATION)\n",
    "# ---------------------------\n",
    "def train_and_evaluate():\n",
    "    X, y, groups = prepare_ml_data(DATA_PATH)\n",
    "    print(f\"Feature Matrix Shape: {X.shape}\")\n",
    "    \n",
    "    # Gunakan RobustScaler karena fitur statistik sering punya outlier\n",
    "    scaler = RobustScaler() \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Simpan Scaler\n",
    "    joblib.dump(scaler, 'scaler_ml.pkl')\n",
    "    \n",
    "    # --- PILIHAN MODEL ---\n",
    "    \n",
    "    # 1. SVM (Support Vector Machine)\n",
    "    svm_model = SVC(\n",
    "        kernel='rbf', \n",
    "        C=10,             # Regularization parameter\n",
    "        gamma='scale',    # Kernel coefficient\n",
    "        probability=True, # Agar bisa predict_proba untuk ensemble\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 3. KNN\n",
    "    knn_model = KNeighborsClassifier(\n",
    "        n_neighbors=7,\n",
    "        weights='distance', # Memberi bobot lebih pada tetangga terdekat\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # --- MODEL UTAMA (ENSEMBLE) ---\n",
    "    # Menggabungkan SVM, Random Forest, dan KNN menggunakan VotingClassifier\n",
    "    # Voting 'soft' menggunakan rata-rata probabilitas prediksi.\n",
    "    \n",
    "    model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', svm_model), \n",
    "            ('rf', rf_model),\n",
    "            ('knn', knn_model)\n",
    "        ],\n",
    "        voting='soft',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"Training using: {model.__class__.__name__} (SVM + RF + KNN)\")\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    scores = []\n",
    "    models_list = []\n",
    "    \n",
    "    print(\"\\nStarting Cross-Validation...\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_scaled, y, groups)):\n",
    "        X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "        X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, preds, average='macro')\n",
    "        scores.append(f1)\n",
    "        \n",
    "        print(f\"Fold {fold+1} F1: {f1:.4f}\")\n",
    "        # print(classification_report(y_val, preds, target_names=['Non', 'Drowsy']))\n",
    "        \n",
    "        # Simpan model\n",
    "        joblib.dump(model, f'model_ensemble_fold_{fold+1}.pkl')\n",
    "        models_list.append(f'model_ensemble_fold_{fold+1}.pkl') \n",
    "        \n",
    "    print(f\"\\nüèÜ Average CV Score: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "    \n",
    "    return models_list, scaler\n",
    "\n",
    "# ---------------------------\n",
    "# 5. INFERENCE ON TEST DATA\n",
    "# ---------------------------\n",
    "def run_test_inference(model_paths, scaler):\n",
    "    if not os.path.exists(TEST_NPZ_PATH):\n",
    "        print(f\"‚ùå Test data not found at: {TEST_NPZ_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nLoading Test Data from {TEST_NPZ_PATH}...\")\n",
    "    data = np.load(TEST_NPZ_PATH)\n",
    "    \n",
    "    # Handle variasi nama key pada file npz (kadang X_test, kadang X_test_num)\n",
    "    if 'X_test' in data:\n",
    "        X_test_seq = data['X_test']\n",
    "    elif 'X_test_num' in data:\n",
    "        X_test_seq = data['X_test_num']\n",
    "    else:\n",
    "        # Fallback coba ambil array pertama\n",
    "        X_test_seq = data[data.files[0]]\n",
    "        \n",
    "    video_ids = data['ids'] if 'ids' in data else data['video_ids']\n",
    "    \n",
    "    print(\"Extracting Test Features...\")\n",
    "    X_test_ml = []\n",
    "    for seq in tqdm(X_test_seq):\n",
    "        stats = extract_stats(seq)\n",
    "        X_test_ml.append(stats)\n",
    "    X_test_ml = np.array(X_test_ml)\n",
    "    \n",
    "    # Scaling\n",
    "    X_test_scaled = scaler.transform(X_test_ml)\n",
    "    \n",
    "    # Ensemble Prediction (Average Probability dari semua Fold)\n",
    "    print(\"Predicting with Ensemble of CV Models...\")\n",
    "    final_probs = np.zeros((len(X_test_scaled), 2))\n",
    "    \n",
    "    for m_path in model_paths:\n",
    "        model = joblib.load(m_path)\n",
    "        # VotingClassifier mendukung predict_proba jika voting='soft'\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X_test_scaled)\n",
    "        else:\n",
    "            p = model.predict(X_test_scaled)\n",
    "            probs = np.zeros((len(p), 2))\n",
    "            for i, val in enumerate(p): probs[i, val] = 1.0\n",
    "        final_probs += probs\n",
    "        \n",
    "    final_preds = np.argmax(final_probs, axis=1)\n",
    "    \n",
    "    # Save CSV\n",
    "    df = pd.DataFrame({'video_id': video_ids, 'label': final_preds})\n",
    "    # Cleaning ID\n",
    "    df['video_id'] = df['video_id'].astype(str).str.replace('.mp4', '', regex=False)\n",
    "    df = df.sort_values('video_id')\n",
    "    \n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Submission Saved: {OUTPUT_CSV}\")\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Train Ensemble (SVM+RF+KNN)\n",
    "    trained_model_paths, fitted_scaler = train_and_evaluate()\n",
    "    \n",
    "    # 2. Predict Test Data\n",
    "    run_test_inference(trained_model_paths, fitted_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43c7140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading numeric_cv_data_non2/numeric_data_cv.npz...\n",
      "Extracting Statistical Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3189/3189 [00:00<00:00, 4320.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix Shape: (3189, 34)\n",
      "Training using: VotingClassifier\n",
      "\n",
      "Starting Cross-Validation...\n",
      "Fold 1 F1: 0.7659\n",
      "Fold 2 F1: 0.7854\n",
      "Fold 3 F1: 0.8650\n",
      "Fold 4 F1: 0.7535\n",
      "Fold 5 F1: 0.8391\n",
      "\n",
      "üèÜ Average CV Score: 0.8018 (+/- 0.0431)\n",
      " Test data not found at: test_data_numeric_output/test_numeric_sliding.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "# ---------------------------\n",
    "# 1. KONFIGURASI\n",
    "# ---------------------------\n",
    "DATA_PATH = \"numeric_cv_data_non2/numeric_data_cv.npz\" \n",
    "TEST_NPZ_PATH = \"test_data_numeric_output/test_numeric_sliding.npz\" \n",
    "OUTPUT_CSV = \"submission_svm3.csv\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2. FEATURE ENGINEERING (STATISTIKAL)\n",
    "# ---------------------------\n",
    "def extract_stats(seq_data):\n",
    "    \"\"\"\n",
    "    Mengubah sequence (30, 12) menjadi vector statistik (1, N_Features).\n",
    "    seq_data shape: (30, 12)\n",
    "    Fitur: Mean, Std, Max, Min, Range, Blink Count, Microsleep Duration\n",
    "    \"\"\"\n",
    "    # 12 Fitur Awal:\n",
    "    # 0:EAR_L, 1:EAR_R, 2:MAR, 3:Perclos, 4:Blink, 5:Yawn, \n",
    "    # 6-8:Pose, 9:Face, 10:Nod, 11:Gaze\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Ambil fitur kunci saja untuk statistik\n",
    "    ear_avg = (seq_data[:, 0] + seq_data[:, 1]) / 2.0\n",
    "    mar = seq_data[:, 2]\n",
    "    pitch = seq_data[:, 6]\n",
    "    yaw = seq_data[:, 7]\n",
    "    roll = seq_data[:, 8]\n",
    "    gaze = seq_data[:, 11]\n",
    "    \n",
    "    # Kumpulan sinyal yang akan dianalisis\n",
    "    signals = [ear_avg, mar, pitch, yaw, roll, gaze]\n",
    "    \n",
    "    for sig in signals:\n",
    "        features.append(np.mean(sig))\n",
    "        features.append(np.std(sig))\n",
    "        features.append(np.max(sig))\n",
    "        features.append(np.min(sig))\n",
    "        features.append(np.max(sig) - np.min(sig)) # Range\n",
    "    \n",
    "    # --- FITUR KHUSUS (DOMAIN KNOWLEDGE) ---\n",
    "    \n",
    "    # 1. Blink Count (Turun naik) - Hitung berapa kali EAR turun dibawah 0.20\n",
    "    # Menggunakan diff untuk menghitung transisi open->close\n",
    "    binary_ear = (ear_avg < 0.20).astype(int)\n",
    "    transitions = np.diff(binary_ear)\n",
    "    blink_count = np.sum(transitions == 1) # Hitung transisi masuk ke fase tertutup\n",
    "    features.append(blink_count)\n",
    "    \n",
    "    # 2. Microsleep Duration (Max consecutive frames mata tertutup)\n",
    "    closed_frames = ear_avg < 0.20\n",
    "    max_closed = 0\n",
    "    current_closed = 0\n",
    "    for is_closed in closed_frames:\n",
    "        if is_closed:\n",
    "            current_closed += 1\n",
    "        else:\n",
    "            max_closed = max(max_closed, current_closed)\n",
    "            current_closed = 0\n",
    "    max_closed = max(max_closed, current_closed)\n",
    "    features.append(max_closed)\n",
    "    \n",
    "    # 3. Yawn Duration (MAR > 0.5)\n",
    "    yawn_frames = np.sum(mar > 0.5)\n",
    "    features.append(yawn_frames)\n",
    "    \n",
    "    # 4. Head Nodding (Pitch > Threshold secara konsisten)\n",
    "    nodding_frames = np.sum(pitch > 15)\n",
    "    features.append(nodding_frames)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. PREPARE DATASET FOR ML\n",
    "# ---------------------------\n",
    "def prepare_ml_data(npz_path):\n",
    "    print(f\"Loading {npz_path}...\")\n",
    "    data = np.load(npz_path)\n",
    "    X_seq = data['X'] # (N, 30, 12)\n",
    "    y = data['y']\n",
    "    groups = data['groups']\n",
    "    \n",
    "    X_ml = []\n",
    "    print(\"Extracting Statistical Features...\")\n",
    "    for seq in tqdm(X_seq):\n",
    "        stats = extract_stats(seq)\n",
    "        X_ml.append(stats)\n",
    "        \n",
    "    return np.array(X_ml), y, groups\n",
    "\n",
    "# ---------------------------\n",
    "# 4. TRAINING (CROSS-VALIDATION)\n",
    "# ---------------------------\n",
    "def train_and_evaluate():\n",
    "    X, y, groups = prepare_ml_data(DATA_PATH)\n",
    "    print(f\"Feature Matrix Shape: {X.shape}\")\n",
    "    \n",
    "    # Menggunakan RobustScaler karena fitur statistik sering punya outlier\n",
    "    scaler = RobustScaler() \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Simpan Scaler\n",
    "    joblib.dump(scaler, 'scaler_ml.pkl')\n",
    "    \n",
    "    # --- PILIHAN MODEL ---\n",
    "    \n",
    "    # 1. SVM (Support Vector Machine) - SANGAT DIREKOMENDASIKAN\n",
    "    # class_weight='balanced' penting untuk dataset kecil/imbalance\n",
    "    svm_model = SVC(\n",
    "        kernel='rbf', \n",
    "        C=10,             # Regularization parameter\n",
    "        gamma='scale',    # Kernel coefficient\n",
    "        probability=True, # Agar bisa predict_proba untuk ensemble\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 2. Random Forest - Sebagai pembanding/Ensemble\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # --- MODEL UTAMA YANG AKAN DILATIH ---\n",
    "    \n",
    "    model = VotingClassifier(\n",
    "        estimators=[('svm', svm_model), ('rf', rf_model)],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    print(f\"Training using: {model.__class__.__name__}\")\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    scores = []\n",
    "    models_list = []\n",
    "    \n",
    "    print(\"\\nStarting Cross-Validation...\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_scaled, y, groups)):\n",
    "        X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "        X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, preds, average='macro')\n",
    "        scores.append(f1)\n",
    "        \n",
    "        print(f\"Fold {fold+1} F1: {f1:.4f}\")\n",
    "        # print(classification_report(y_val, preds, target_names=['Non', 'Drowsy']))\n",
    "        \n",
    "        # Simpan model (Kita butuh clone fresh model atau fit ulang, \n",
    "        joblib.dump(model, f'model_ml_fold_{fold+1}.pkl')\n",
    "        models_list.append(f'model_ml_fold_{fold+1}.pkl') # Simpan path/nama\n",
    "        \n",
    "    print(f\"\\nüèÜ Average CV Score: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "    \n",
    "    return models_list, scaler\n",
    "\n",
    "# ---------------------------\n",
    "# 5. INFERENCE ON TEST DATA\n",
    "# ---------------------------\n",
    "def run_test_inference(model_paths, scaler):\n",
    "    if not os.path.exists(TEST_NPZ_PATH):\n",
    "        print(f\" Test data not found at: {TEST_NPZ_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nLoading Test Data from {TEST_NPZ_PATH}...\")\n",
    "    data = np.load(TEST_NPZ_PATH)\n",
    "    \n",
    "    # Handle variasi nama key pada file npz (kadang X_test, kadang X_test_num)\n",
    "    if 'X_test' in data:\n",
    "        X_test_seq = data['X_test']\n",
    "    elif 'X_test_num' in data:\n",
    "        X_test_seq = data['X_test_num']\n",
    "    else:\n",
    "        # Fallback coba ambil array pertama\n",
    "        X_test_seq = data[data.files[0]]\n",
    "        \n",
    "    video_ids = data['ids'] if 'ids' in data else data['video_ids']\n",
    "    \n",
    "    print(\"Extracting Test Features...\")\n",
    "    X_test_ml = []\n",
    "    for seq in tqdm(X_test_seq):\n",
    "        stats = extract_stats(seq)\n",
    "        X_test_ml.append(stats)\n",
    "    X_test_ml = np.array(X_test_ml)\n",
    "    \n",
    "    # Scaling\n",
    "    X_test_scaled = scaler.transform(X_test_ml)\n",
    "    \n",
    "    # Ensemble Prediction (Average Probability dari semua Fold)\n",
    "    print(\"Predicting with Ensemble of CV Models...\")\n",
    "    final_probs = np.zeros((len(X_test_scaled), 2))\n",
    "    \n",
    "    for m_path in model_paths:\n",
    "        model = joblib.load(m_path)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X_test_scaled)\n",
    "        else:\n",
    "            p = model.predict(X_test_scaled)\n",
    "            probs = np.zeros((len(p), 2))\n",
    "            for i, val in enumerate(p): probs[i, val] = 1.0\n",
    "        final_probs += probs\n",
    "        \n",
    "    final_preds = np.argmax(final_probs, axis=1)\n",
    "    \n",
    "    # Save CSV\n",
    "    df = pd.DataFrame({'video_id': video_ids, 'label': final_preds})\n",
    "    # Cleaning ID\n",
    "    df['video_id'] = df['video_id'].astype(str).str.replace('.mp4', '', regex=False)\n",
    "    df = df.sort_values('video_id')\n",
    "    \n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Submission Saved: {OUTPUT_CSV}\")\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Train SVM/RF\n",
    "    trained_model_paths, fitted_scaler = train_and_evaluate()\n",
    "    \n",
    "    # 2. Predict Test Data\n",
    "    run_test_inference(trained_model_paths, fitted_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
